{"cells":[{"cell_type":"markdown","metadata":{"id":"k-KdVKbkpmdo"},"source":["# Importación de librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7S1UZ7Rpmds"},"outputs":[],"source":["import os\n","from scipy.io import loadmat\n","import numpy as np\n","import torch as th\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import math\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch.autograd import Variable"]},{"cell_type":"markdown","metadata":{"id":"_tqWJ60rpmdu"},"source":["# Carga de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dehVhGtupmdu"},"outputs":[],"source":["DATA_PATH = 'shhs3'\n","dbpath=os.path.abspath(f'{DATA_PATH}')\n","listdir = os.listdir(dbpath)\n","listdir.sort()\n","listdir = listdir[1:]\n","lengths = loadmat(os.path.join(dbpath, 'sequenceLengths.mat'))\n","lengths = lengths['sequenceLengths'].squeeze()\n","\n","subj = []\n","for i in range(len(listdir)):\n","    subj.append(loadmat(os.path.join(dbpath, listdir[i])))\n","\n","feat_data = [] # SaO2\n","target_data = [] # TargetA0H4\n","\n","for i in range(len(subj)):\n","        feat_data.extend(subj[i]['SaO2'].reshape(1, -1))\n","        target_data.append(subj[i]['targetA0H3'].flatten())\n","\n","feat_data = np.concatenate(feat_data)\n","target_data = np.concatenate(target_data)\n","\n","for i in range(len(target_data)):\n","    if target_data[i] > 1:\n","        target_data[i] = 1\n","\n","def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n","    nrows = ((a.size - L) // S) + 1\n","    n = a.strides[0]\n","    return np.lib.stride_tricks.as_strided(\n","        a, shape=(nrows, L), strides=(S * n, n))\n","\n","window = 200\n","stride = 1\n","\n","feat_windowed_data = strided_app(feat_data, window, stride)\n","target_windowed_data = strided_app(target_data, window, stride)\n","\n","dataset = TensorDataset(\n","        th.from_numpy(feat_windowed_data),\n","        th.from_numpy(target_windowed_data),\n","    )\n","\n","generator = th.Generator().manual_seed(42)\n","traindata, valdata, testdata = th.utils.data.random_split(dataset, [0.7, 0.2, 0.1], generator=generator)\n","\n","trainloader = DataLoader(\n","       traindata, batch_size=512, pin_memory=False, drop_last=False, shuffle=True\n","    )\n","\n","valloader = DataLoader(\n","       valdata, batch_size=512, pin_memory=False, drop_last=False, shuffle=True\n","    )\n","\n","testloader = DataLoader(\n","       testdata, batch_size=512, pin_memory=False, drop_last=False, shuffle=True\n","    )"]},{"cell_type":"markdown","metadata":{"id":"C86Bv34ppmdv"},"source":["# Definición del algoritmo\n","\n","Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJI20kGTpmdv"},"outputs":[],"source":["class AutoencoderAp(nn.Module):\n","    def __init__(self, window=100, dropout=0.2):\n","        super().__init__()\n","\n","        self.c1 = nn.Sequential(\n","            nn.Linear(200, 175),\n","            nn.BatchNorm1d(175),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(175, 125),\n","            nn.BatchNorm1d(125),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(125, 100),\n","            nn.BatchNorm1d(100),\n","            nn.Tanh(),\n","        )\n","\n","        self.feature_extractor = nn.Sequential(\n","            nn.Linear(100, 125),\n","            nn.BatchNorm1d(125),\n","            nn.ReLU(),\n","\n","            nn.Linear(125, 175),\n","            nn.BatchNorm1d(175),\n","            nn.ReLU(),\n","\n","            nn.Linear(175, 200),\n","            nn.Sigmoid(),\n","        )\n","\n","       # self._initialize_submodules()\n","\n","    def _initialize_submodules(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                # init.kaiming_normal(m.weight.data)\n","                n = m.weight.size(1)\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","            elif isinstance(m, nn.Conv1d):\n","                # n = m.kernel_size[0] * m.out_channels\n","                n = m.kernel_size[0] * m.in_channels\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","\n","    def forward(self, x):\n","        y = self.c1(x)\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"IqGN4c2hpmdw"},"source":["Generador y discriminador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPpTkqcmpmdw"},"outputs":[],"source":["class AutoencoderSaO2(nn.Module):\n","    def __init__(self, window=100, dropout=0.5):\n","        super().__init__()\n","\n","\n","        self.c1 = nn.Sequential(\n","            nn.Linear(200, 175),\n","            nn.BatchNorm1d(175),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(175, 125),\n","            nn.BatchNorm1d(125),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(125, 100),\n","            nn.BatchNorm1d(100),\n","            nn.Tanh(),\n","        )\n","\n","        self.feature_extractor = nn.Sequential(\n","            nn.Linear(100, 125),\n","            nn.BatchNorm1d(125),\n","            nn.ReLU(),\n","\n","            nn.Linear(125, 175),\n","            nn.BatchNorm1d(175),\n","            nn.ReLU(),\n","\n","            nn.Linear(175, 200),\n","            nn.Sigmoid(),\n","        )\n","\n","        self._initialize_submodules()\n","\n","    def _initialize_submodules(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","               # init.kaiming_normal(m.weight.data)\n","                n = m.weight.size(1)\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","            elif isinstance(m, nn.Conv1d):\n","                # n = m.kernel_size[0] * m.out_channels\n","                n = m.kernel_size[0] * m.in_channels\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","\n","    def forward(self, x):\n","        # Feature extractor\n","        embeddings_ = self.c1(x)\n","        y = self.feature_extractor(embeddings_)\n","\n","        return y, embeddings_\n","\n","class Discriminador(nn.Module):\n","    def __init__(self, window=50, dropout=0.5):\n","        super().__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Linear(100, 50),\n","            nn.BatchNorm1d(50),\n","            nn.ReLU(),\n","            nn.Linear(50, 25),\n","            nn.BatchNorm1d(25),\n","            nn.ReLU(),\n","            nn.Linear(25, 1),\n","            nn.BatchNorm1d(1),\n","            nn.Sigmoid(),\n","        )\n","\n","        self._initialize_submodules()\n","\n","    def _initialize_submodules(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                # init.kaiming_normal(m.weight.data)\n","                n = m.weight.size(1)\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","            elif isinstance(m, nn.Conv1d):\n","                # n = m.kernel_size[0] * m.out_channels\n","                n = m.kernel_size[0] * m.in_channels\n","                m.weight.data.normal_(0, math.sqrt(1.0 / n))\n","\n","    def forward(self, x):\n","        y = self.feature_extractor(x)\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"yxArRbivpmdx"},"source":["Carga de modelos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0IHf8expmdx"},"outputs":[],"source":["modelSaO2 = AutoencoderSaO2()\n","modelAp = AutoencoderAp()\n","discriminador = Discriminador()\n","\n","# CARGA AUTOENCODER YA ENTRENADO\n","modelAp.load_state_dict(th.load('models/modelAp.pth'))"]},{"cell_type":"markdown","metadata":{"id":"uqhMTNpopmdy"},"source":["# Optimizadores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odYiNF3jpmdy"},"outputs":[],"source":["optimizer_G = optim.Adam(list(modelSaO2.parameters())[:12], lr=2e-4)\n","optimizer_R = optim.Adam(modelSaO2.parameters(), lr=2e-4)\n","optimizer_D = optim.Adam(discriminador.parameters(), lr=2e-4)\n","\n","scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, 10, 0.9)\n","scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, 10, 0.9)\n","scheduler_R = optim.lr_scheduler.StepLR(optimizer_D, 10, 0.5)"]},{"cell_type":"markdown","metadata":{"id":"PRs9wiYipmdy"},"source":["# Entrenamiento y validación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idM5KxgKpmdy"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n","\n","num_epochs = 50\n","\n","modelAp.to(device)\n","modelSaO2.to(device)\n","discriminador.to(device)\n","\n","for epoch in range(num_epochs):\n","\n","    # ENTRENAMIENTO\n","\n","    modelSaO2.train()\n","    discriminador.train()\n","    modelAp.eval()\n","\n","    loss_train_epoch = 0.0\n","    D_loss_sum = 0\n","    D_real_prob = 0\n","    D_fake_prob = 0\n","    SaO2_loss_sum = 0\n","    loss_sum = 0\n","    reconstruction_loss = 0\n","\n","    # Itero en los batch de trainloader\n","    for SaO2_data , Ap_data  in trainloader:\n","\n","      SaO2_data , Ap_data = SaO2_data.to(device) , Ap_data.to(device)\n","\n","      optimizer_SaO2.zero_grad()\n","      optimizer_Ap.zero_grad()\n","      optimizer_D.zero_grad()\n","\n","      batch_size = SaO2_data.shape[0]\n","      ones_label = th.ones(batch_size, 1).to(device)\n","      zeros_label = th.zeros(batch_size, 1).to(device)\n","\n","      # Optimización del discriminador\n","      true = modelAp(Ap_data.to(th.float32))\n","      _, embeddings_ = modelSaO2(SaO2_data.to(th.float32))\n","      D_real = discriminador(true)\n","      D_fake = discriminador(embeddings_)\n","\n","      D_loss_real = F.binary_cross_entropy(D_real, ones_label)\n","      D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label)\n","      D_loss = D_loss_real + D_loss_fake\n","\n","      D_real_prob += D_real.mean().item()\n","      D_fake_prob += D_fake.mean().item()\n","\n","      D_loss.backward()\n","      optimizer_D.step()\n","      optimizer_SaO2.zero_grad()\n","      optimizer_D.zero_grad()\n","      optimizer_Ap.zero_grad()\n","\n","      # Optimización del generador\n","      _, embeddings_ = modelSaO2(SaO2_data.to(th.float32))\n","      D_fake = discriminador(embeddings_)\n","\n","      SaO2_loss = F.binary_cross_entropy(D_fake, ones_label)\n","      SaO2_loss.backward()\n","      optimizer_SaO2.step()\n","      optimizer_SaO2.zero_grad()\n","      optimizer_D.zero_grad()\n","      optimizer_Ap.zero_grad()\n","\n","      reconstructions, _ = modelSaO2(SaO2_data.to(th.float32))\n","\n","      loss_reconstruction = F.l1_loss(Ap_data,reconstructions)\n","      loss_reconstruction.backward()\n","      optimizer_Ap.step()\n","      optimizer_SaO2.zero_grad()\n","      optimizer_D.zero_grad()\n","      optimizer_Ap.zero_grad()\n","\n","      net_loss = loss_reconstruction + SaO2_loss\n","\n","      loss_sum += net_loss.item()\n","      reconstruction_loss += loss_reconstruction.item()\n","\n","      D_loss_sum += D_loss\n","      SaO2_loss_sum += SaO2_loss\n","\n","    scheduler_SaO2.step()\n","\t  scheduler_Ap.step()\n","\t  scheduler_D.step()\n","\n","    path_save = 'models/'\n","\t  th.save(modelSaO2.state_dict(), os.path.join(path_save,f'modelSaO2_epoch_{epoch}.pth'))\n","\t  th.save(discriminador.state_dict(), os.path.join(path_save, f'discriminador_epoch_{epoch}.pth'))\n","\n","    # VALIDACIÓN\n","\n","    modelSaO2.eval()\n","    discriminador.eval()\n","    modelAp.eval()\n","\n","    D_loss_sum = 0\n","    SaO2_loss_sum = 0\n","    loss_sum = 0\n","    D_real_prob = 0\n","    D_fake_prob = 0\n","    reconstruction_loss = 0\n","\n","    with th.no_grad():\n","\n","      # Itero en los batch de valloader\n","      for SaO2_data , Ap_data in valloader:\n","\n","          SaO2_data, Ap_data = SaO2_data.to(device), Ap_data.to(device)\n","          SaO2_data.requires_grad_, Ap_data.requires_grad_ = False, False\n","\n","          batch_size = SaO2_data.shape[0]\n","          ones_label = th.ones(batch_size, 1).to(device)\n","          zeros_label = th.zeros(batch_size, 1).to(device)\n","\n","          # Test discriminador\n","          true = modelAp(Ap_data.to(th.float32))\n","          _, embeddings_ = modelSaO2(SaO2_data.to(th.float32))\n","          D_real = discriminador(true)\n","          D_fake = discriminador(embeddings_)\n","\n","          D_loss_real = F.binary_cross_entropy(D_real, ones_label)\n","          D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label)\n","          D_loss = D_loss_real + D_loss_fake\n","\n","          D_real_prob += D_real.mean().item()\n","          D_fake_prob += D_fake.mean().item()\n","\n","          # Test generador\n","          reconstructions, embeddings_ = modelSaO2(SaO2_data.to(th.float32))\n","          D_fake = discriminador(embeddings_)\n","\n","          loss_reconstruction = F.l1_loss(Ap_data, reconstructions)\n","\n","          SaO2_loss = F.binary_cross_entropy(D_fake, ones_label)\n","\n","          net_loss = loss_reconstruction + SaO2_loss\n","\n","          loss_sum += net_loss.item()\n","          reconstruction_loss += loss_reconstruction.item()\n","\n","          D_loss_sum += D_loss\n","          SaO2_loss_sum += SaO2_loss"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}